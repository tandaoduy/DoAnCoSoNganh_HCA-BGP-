"""Step 5/6: core-cluster merging (MPNN) and final K-means clustering.

This module implements the MPNN-based merging of core-clusters (Step 5)
and a custom K-means run initialized from core-cluster centroids (Step 6).
It also computes clustering quality metrics such as Silhouette and
Davies-Bouldin indices.
"""

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import time
import math
from utils import euclid
from step4_core_grouping import (
    build_core_clusters,
    compute_initial_centroids,
    plot_core_groups,
)

# B·∫£ng m√†u d√πng chung cho t·ªëi ƒëa ~100 cluster
_cmap_tab20 = plt.get_cmap("tab20")
_cmap_tab20c = plt.get_cmap("tab20c")
_cmap_hsv = plt.get_cmap("hsv")

CLUSTER_COLORS = [
    _cmap_tab20(i / 20.0) for i in range(20)
] + [
    _cmap_tab20c(i / 20.0) for i in range(20)
] + [
    _cmap_hsv(i / 60.0) for i in range(60)
]


# ------------------------------------------------------
# Ph√¢n c·ª•m b·∫±ng K-means v·ªõi t√¢m kh·ªüi t·∫°o = core-centroids
# (C√†i tay, d√πng utils.euclid l√†m metric)
# ------------------------------------------------------
def kmeans_assign_all_points_custom(data, init_centroids, max_iter=100, tol=1e-4):
    data = np.asarray(data)
    centroids = np.asarray(init_centroids, dtype=float).copy()
    n_samples = data.shape[0]
    k = centroids.shape[0]

    # Nh√£n c·ª•m cho t·ª´ng ƒëi·ªÉm
    labels = np.zeros(n_samples, dtype=int)
    n_iter = 0

    for it in range(max_iter):
        # 1) G√°n t·ª´ng ƒëi·ªÉm v√†o centroid g·∫ßn nh·∫•t (d√πng euclid t·ª´ utils)
        changed = False
        for i in range(n_samples):
            p = data[i]
            # t√≠nh kho·∫£ng c√°ch t·ªõi t·ª´ng centroid
            dists = [euclid(p, centroids[j]) for j in range(k)]
            for j in range(k):
                c = centroids[j]
                d = dists[j]

                # ƒê·ªãnh d·∫°ng to·∫° ƒë·ªô ƒëi·ªÉm p v√† centroid c v·ªõi 2 ch·ªØ s·ªë th·∫≠p ph√¢n ƒë·ªÉ log g·ªçn h∆°n
                px, py = float(p[0]), float(p[1])
                cx, cy = float(c[0]), float(c[1])
                print(
                    f"[Step 6 - K-means] d(p{i}, c{j}) = "
                    f"euclid(p=({px:.2f}, {py:.2f}), c=({cx:.2f}, {cy:.2f})) = {d:.2f}"
                )
            best_label = int(np.argmin(dists))
            if labels[i] != best_label:
                labels[i] = best_label
                changed = True

        # N·∫øu kh√¥ng ƒëi·ªÉm n√†o ƒë·ªïi c·ª•m n·ªØa th√¨ coi nh∆∞ h·ªôi t·ª•
        if not changed:
            print(f"[K-means] D·ª´ng s·ªõm v√¨ kh√¥ng c√≥ ƒëi·ªÉm n√†o ƒë·ªïi c·ª•m. Iter {it + 1}")
            break

        # 2) C·∫≠p nh·∫≠t l·∫°i centroid t·ª´ng c·ª•m
        new_centroids = centroids.copy()
        for j in range(k):
            cluster_points = data[labels == j]
            if len(cluster_points) > 0:
                new_centroids[j] = cluster_points.mean(axis=0)

        # Ki·ªÉm tra ƒë·ªô d·ªãch chuy·ªÉn c·ªßa centroids ƒë·ªÉ d·ª´ng s·ªõm
        shift = np.linalg.norm(new_centroids - centroids)
        centroids = new_centroids
        # In th√¥ng tin t·ª´ng v√≤ng l·∫∑p K-means
        cluster_sizes = [int(np.sum(labels == j)) for j in range(k)]
        n_iter = it + 1
        print(f"[K-means] Iter {n_iter}: shift={shift:.2f}, sizes={cluster_sizes}")
        if shift < tol:
            print("[K-means] D·ª´ng s·ªõm v√¨ shift < tol.")
            break

    print(f"[K-means] K·∫øt th√∫c sau {n_iter} v√≤ng l·∫∑p.")
    return labels, centroids


# ------------------------------------------------------
# T√≠nh Silhouette cho to√†n b·ªô ƒëi·ªÉm (d√πng kho·∫£ng c√°ch euclid)
# ------------------------------------------------------
def compute_silhouette(points, labels):
    """Tr·∫£ v·ªÅ (silhouette_t·ª´ng_ƒëi·ªÉm, silhouette_trung_b√¨nh)."""
    data = np.asarray(points)
    labels = np.asarray(labels)
    n = len(data)
    if n == 0:
        return np.array([]), 0.0

    unique_clusters = np.unique(labels)
    if len(unique_clusters) < 2:
        # Silhouette kh√¥ng c√≥ √Ω nghƒ©a n·∫øu ch·ªâ c√≥ 1 c·ª•m
        return np.zeros(n, dtype=float), 0.0

    # Ma tr·∫≠n kho·∫£ng c√°ch n x n
    D = np.zeros((n, n), dtype=float)
    for i in range(n):
        for j in range(i + 1, n):
            d = euclid(data[i], data[j])
            D[i, j] = D[j, i] = d

    s = np.zeros(n, dtype=float)
    indices = np.arange(n)
    for i in range(n):
        ci = labels[i]
        same = (labels == ci)
        # Lo·∫°i b·ªè ch√≠nh ƒëi·ªÉm i kh·ªèi mask c√πng c·ª•m
        same_no_i = np.logical_and(same, indices != i)
        # N·∫øu c·ª•m ch·ªâ c√≥ 1 ƒëi·ªÉm -> silhouette = 0
        if not np.any(same_no_i):
            s[i] = 0.0
            continue

        # a(i): kho·∫£ng c√°ch trung b√¨nh t·ªõi c√°c ƒëi·ªÉm c√πng c·ª•m
        a_i = D[i, same_no_i].mean()

        # b(i): kho·∫£ng c√°ch trung b√¨nh nh·ªè nh·∫•t t·ªõi c·ª•m kh√°c
        b_i = float("inf")
        for c in unique_clusters:
            if c == ci:
                continue
            other = (labels == c)
            if not np.any(other):
                continue
            d_c = D[i, other].mean()
            if d_c < b_i:
                b_i = d_c

        s[i] = (b_i - a_i) / max(a_i, b_i) if b_i > 0 else 0.0

    return s, float(s.mean())


def compute_davies_bouldin(points, labels, centroids):
    """Compute Davies-Bouldin index for clustering using Euclidean distance.

    Returns a float (DB index). If there are fewer than 2 non-empty clusters,
    returns float('inf').
    """
    data = np.asarray(points)
    labels = np.asarray(labels)
    centroids = np.asarray(centroids)

    # Identify non-empty clusters (by label index)
    unique_labels = np.unique(labels)
    # If centroids are given as list/array indexed by label, consider labels 0..len(centroids)-1
    # But evaluate only those labels that actually have members
    valid_clusters = [int(l) for l in unique_labels if np.sum(labels == l) > 0]

    if len(valid_clusters) < 2:
        print("[DB] Kh√¥ng ƒë·ªß c·ª•m (>=2) ƒë·ªÉ t√≠nh Davies-Bouldin. Tr·∫£ v·ªÅ inf.")
        return float("inf")

    # Compute S_i: average distance of points in cluster i to its centroid
    S = {}
    for i in valid_clusters:
        members = data[labels == i]
        if len(members) == 0:
            S[i] = 0.0
        else:
            # use euclid for robust 2D/ND distance
            dists = [euclid(p, centroids[i]) for p in members]
            S[i] = float(np.mean(dists)) if len(dists) > 0 else 0.0

    # Compute pairwise centroid distances and R_ij
    D_i = []
    for i in valid_clusters:
        max_r = 0.0
        for j in valid_clusters:
            if j == i:
                continue
            dist_c = euclid(centroids[i], centroids[j])
            if dist_c <= 0:
                r_ij = float("inf")
            else:
                r_ij = (S[i] + S[j]) / dist_c
            if r_ij > max_r:
                max_r = r_ij
        D_i.append(max_r)

    # DB index is mean of D_i
    db_index = float(np.mean(D_i)) if len(D_i) > 0 else float("inf")
    print(f"[DB] Davies-Bouldin index = {db_index:.6f}")
    return db_index


# =============================
# IN CHI TI·∫æT C√îNG TH·ª®C T√çNH TO√ÅN K-MEANS, EUCLIDEAN, SILHOUETTE, DAVIES-BOULDIN
# =============================
def print_kmeans_formulas_detail(points, init_centroids, max_iter=100, tol=1e-4):
    """
    In chi ti·∫øt t·ª´ng b∆∞·ªõc t√≠nh to√°n K-means v·ªõi c√¥ng th·ª©c Euclidean,
    r·ªìi t√≠nh Silhouette v√† Davies-Bouldin sau khi h·ªôi t·ª•.
    """
    data = np.asarray(points)
    centroids = np.asarray(init_centroids, dtype=float).copy()
    n_samples = data.shape[0]
    k = centroids.shape[0]
    
    print("\n" + "="*100)
    print("CHI TI·∫æT C√îNG TH·ª®C T√çNH TO√ÅN K-MEANS V√Ä C√ÅC CH·ªà S·ªê ƒê√ÅNH GI√Å")
    print("="*100)
    
    # =============================================
    # PH·∫¶N 1: C√îNG TH·ª®C EUCLIDEAN DISTANCE
    # =============================================
    print("\n" + "‚îÄ"*100)
    print("üìê C√îNG TH·ª®C KHO·∫¢NG C√ÅCH EUCLIDEAN (Euclidean Distance)")
    print("‚îÄ"*100)
    print("\n‚ñ∂ C√îNG TH·ª®C T·ªîNG QU√ÅT:")
    print("   d(p, c) = ‚àö[Œ£·µ¢ (p·µ¢ - c·µ¢)¬≤]")
    print("   Trong ƒë√≥:")
    print("   ‚Ä¢ p = (p‚ÇÅ, p‚ÇÇ, ..., p‚Çô) l√† t·ªça ƒë·ªô ƒëi·ªÉm d·ªØ li·ªáu")
    print("   ‚Ä¢ c = (c‚ÇÅ, c‚ÇÇ, ..., c‚Çô) l√† t·ªça ƒë·ªô t√¢m c·ª•m (centroid)")
    print("   ‚Ä¢ n l√† s·ªë chi·ªÅu d·ªØ li·ªáu")
    print("\n‚ñ∂ V·ªöI D·ªÆ LI·ªÜU 2 CHI·ªÄU (x, y):")
    print("   d(p, c) = ‚àö[(px - cx)¬≤ + (py - cy)¬≤]")
    
    # =============================================
    # PH·∫¶N 2: THU·∫¨T TO√ÅN K-MEANS
    # =============================================
    print("\n" + "‚îÄ"*100)
    print("üîÑ THU·∫¨T TO√ÅN K-MEANS")
    print("‚îÄ"*100)
    print("\n‚ñ∂ B∆Ø·ªöC 1 - KH·ªûI T·∫†O:")
    print(f"   ‚Ä¢ S·ªë ƒëi·ªÉm d·ªØ li·ªáu: n = {n_samples}")
    print(f"   ‚Ä¢ S·ªë c·ª•m mong mu·ªën: K = {k}")
    print("   ‚Ä¢ T√¢m c·ª•m ban ƒë·∫ßu (t·ª´ Core-Cluster centroids):")
    for j, c in enumerate(centroids):
        print(f"     c{j} = ({c[0]:.4f}, {c[1]:.4f})")
    
    labels = np.zeros(n_samples, dtype=int)
    
    for it in range(max_iter):
        print(f"\n‚ñ∂ V√íNG L·∫∂P {it + 1}:")
        print("‚îÄ"*80)
        
        # ========================
        # B∆Ø·ªöC 2A: G√ÅN ƒêI·ªÇM V√ÄO C·ª§M
        # ========================
        print("\n   üìå B∆Ø·ªöC 2A: G√ÅN M·ªñI ƒêI·ªÇM V√ÄO C·ª§M C√ì T√ÇM G·∫¶N NH·∫§T")
        
        changed = False
        for i in range(n_samples):  # In chi ti·∫øt t·∫•t c·∫£ c√°c ƒëi·ªÉm
            p = data[i]
            print(f"\n   üîπ ƒêi·ªÉm {i}: p = ({p[0]:.4f}, {p[1]:.4f})")
            
            dists = []
            for j in range(k):
                c = centroids[j]
                dx = p[0] - c[0]
                dy = p[1] - c[1]
                d = math.sqrt(dx**2 + dy**2)
                dists.append(d)
                
                print(f"      d(p{i}, c{j}) = ‚àö[({p[0]:.4f} - {c[0]:.4f})¬≤ + ({p[1]:.4f} - {c[1]:.4f})¬≤]")
                print(f"                   = ‚àö[{dx**2:.6f} + {dy**2:.6f}]")
                print(f"                   = ‚àö{dx**2 + dy**2:.6f}")
                print(f"                   = {d:.6f}")
            
            best_label = int(np.argmin(dists))
            print(f"      ‚Üí min(d) = {min(dists):.6f} t·∫°i c{best_label}")
            print(f"      ‚Üí G√°n ƒëi·ªÉm {i} v√†o Cluster {best_label}")
            
            if labels[i] != best_label:
                labels[i] = best_label
                changed = True
        
        # ========================
        # B∆Ø·ªöC 2B: C·∫¨P NH·∫¨T T√ÇM C·ª§M
        # ========================
        print("\n   üìå B∆Ø·ªöC 2B: C·∫¨P NH·∫¨T T√ÇM C·ª§M (CENTROID)")
        print("   C√¥ng th·ª©c: c‚±º = (1/|C‚±º|) √ó Œ£(xi), v·ªõi xi ‚àà C‚±º")
        
        new_centroids = centroids.copy()
        for j in range(k):
            cluster_points = data[labels == j]
            n_j = len(cluster_points)
            
            if n_j > 0:
                sum_x = sum(p[0] for p in cluster_points)
                sum_y = sum(p[1] for p in cluster_points)
                new_cx = sum_x / n_j
                new_cy = sum_y / n_j
                new_centroids[j] = np.array([new_cx, new_cy])
                
                print(f"\n   Cluster {j}: {n_j} ƒëi·ªÉm")
                print(f"      T·ªïng X = {sum_x:.4f}, T·ªïng Y = {sum_y:.4f}")
                print(f"      c{j}_new = (1/{n_j}) √ó ({sum_x:.4f}, {sum_y:.4f})")
                print(f"             = ({new_cx:.4f}, {new_cy:.4f})")
        
        # Ki·ªÉm tra h·ªôi t·ª•
        shift = np.linalg.norm(new_centroids - centroids)
        centroids = new_centroids
        
        print(f"\n   üìä T·ªîNG K·∫æT V√íNG L·∫∂P {it + 1}:")
        print(f"      ‚Ä¢ ƒê·ªô d·ªãch chuy·ªÉn t√¢m c·ª•m (shift): {shift:.6f}")
        print(f"      ‚Ä¢ Ng∆∞·ª°ng h·ªôi t·ª• (tol): {tol}")
        
        cluster_sizes = [int(np.sum(labels == j)) for j in range(k)]
        print(f"      ‚Ä¢ Ph√¢n b·ªë ƒëi·ªÉm: {cluster_sizes}")
        
        if not changed:
            print(f"\n   ‚úÖ D·ª™NG: Kh√¥ng c√≥ ƒëi·ªÉm n√†o ƒë·ªïi c·ª•m ‚Üí K-means h·ªôi t·ª•!")
            break
        
        if shift < tol:
            print(f"\n   ‚úÖ D·ª™NG: shift = {shift:.6f} < tol = {tol} ‚Üí K-means h·ªôi t·ª•!")
            break
    
    print(f"\n‚ñ∂ K·∫æT TH√öC K-MEANS SAU {it + 1} V√íNG L·∫∂P")
    print("   T√¢m c·ª•m cu·ªëi c√πng:")
    for j, c in enumerate(centroids):
        cnt = int(np.sum(labels == j))
        print(f"   ‚Ä¢ Cluster {j}: centroid = ({c[0]:.4f}, {c[1]:.4f}), s·ªë ƒëi·ªÉm = {cnt}")
    
    # =============================================
    # PH·∫¶N 3: CH·ªà S·ªê SILHOUETTE
    # =============================================
    print("\n" + "‚îÄ"*100)
    print("üìä CH·ªà S·ªê SILHOUETTE (Silhouette Coefficient)")
    print("‚îÄ"*100)
    
    print("\n‚ñ∂ C√îNG TH·ª®C:")
    print("   s(i) = (b(i) - a(i)) / max(a(i), b(i))")
    print("\n   Trong ƒë√≥:")
    print("   ‚Ä¢ a(i) = kho·∫£ng c√°ch trung b√¨nh t·ª´ ƒëi·ªÉm i ƒë·∫øn c√°c ƒëi·ªÉm C√ôNG c·ª•m")
    print("   ‚Ä¢ b(i) = kho·∫£ng c√°ch trung b√¨nh nh·ªè nh·∫•t t·ª´ ƒëi·ªÉm i ƒë·∫øn c·ª•m KH√ÅC g·∫ßn nh·∫•t")
    print("   ‚Ä¢ s(i) ‚àà [-1, 1]: -1 = ph√¢n c·ª•m sai, 0 = bi√™n, 1 = ph√¢n c·ª•m t·ªët")
    
    # T√≠nh Silhouette cho v√†i ƒëi·ªÉm ƒë·∫ßu
    unique_clusters = np.unique(labels)
    if len(unique_clusters) >= 2:
        # Ma tr·∫≠n kho·∫£ng c√°ch
        D = np.zeros((n_samples, n_samples), dtype=float)
        for i in range(n_samples):
            for j in range(i + 1, n_samples):
                d = euclid(data[i], data[j])
                D[i, j] = D[j, i] = d
        
        print("\n‚ñ∂ T√çNH CHI TI·∫æT CHO T·∫§T C·∫¢ C√ÅC ƒêI·ªÇM:")
        s_values = np.zeros(n_samples)
        
        for i in range(n_samples):  # T√≠nh cho t·∫•t c·∫£ c√°c ƒëi·ªÉm
            ci = labels[i]
            print(f"\n   üîπ ƒêi·ªÉm {i}: p = ({data[i][0]:.4f}, {data[i][1]:.4f}), Cluster = {ci}")
            
            # T√≠nh a(i)
            same = (labels == ci)
            same[i] = False  # lo·∫°i b·ªè ch√≠nh n√≥
            if np.any(same):
                dists_same = D[i, same]
                a_i = float(np.mean(dists_same))
                print(f"      a({i}) = trung b√¨nh kho·∫£ng c√°ch ƒë·∫øn {np.sum(same)} ƒëi·ªÉm c√πng c·ª•m")
                print(f"           = {a_i:.6f}")
            else:
                a_i = 0
                print(f"      a({i}) = 0 (ch·ªâ c√≥ 1 ƒëi·ªÉm trong c·ª•m)")
            
            # T√≠nh b(i)
            b_i = float("inf")
            for c in unique_clusters:
                if c == ci:
                    continue
                other = (labels == c)
                if np.any(other):
                    d_c = float(np.mean(D[i, other]))
                    if d_c < b_i:
                        b_i = d_c
            print(f"      b({i}) = kho·∫£ng c√°ch trung b√¨nh nh·ªè nh·∫•t ƒë·∫øn c·ª•m kh√°c")
            print(f"           = {b_i:.6f}")
            
            # T√≠nh s(i)
            if max(a_i, b_i) > 0:
                s_i = (b_i - a_i) / max(a_i, b_i)
            else:
                s_i = 0
            s_values[i] = s_i
            
            print(f"      s({i}) = ({b_i:.6f} - {a_i:.6f}) / max({a_i:.6f}, {b_i:.6f})")
            print(f"           = {b_i - a_i:.6f} / {max(a_i, b_i):.6f}")
            print(f"           = {s_i:.6f}")
        
        sil_mean = float(np.mean(s_values))
        print(f"\n‚ñ∂ SILHOUETTE TRUNG B√åNH TO√ÄN B·ªò:")
        print(f"   S_avg = (1/n) √ó Œ£ s(i) = {sil_mean:.6f}")
    else:
        sil_mean = 0
        print("\n   ‚ö† Ch·ªâ c√≥ 1 c·ª•m ‚Üí Kh√¥ng t√≠nh ƒë∆∞·ª£c Silhouette")
    
    # =============================================
    # PH·∫¶N 4: CH·ªà S·ªê DAVIES-BOULDIN
    # =============================================
    print("\n" + "‚îÄ"*100)
    print("üìä CH·ªà S·ªê DAVIES-BOULDIN (Davies-Bouldin Index)")
    print("‚îÄ"*100)
    
    print("\n‚ñ∂ C√îNG TH·ª®C:")
    print("   DB = (1/K) √ó Œ£ D·µ¢")
    print("\n   Trong ƒë√≥:")
    print("   ‚Ä¢ D·µ¢ = max(R·µ¢‚±º) v·ªõi j ‚â† i")
    print("   ‚Ä¢ R·µ¢‚±º = (S·µ¢ + S‚±º) / d(c·µ¢, c‚±º)")
    print("   ‚Ä¢ S·µ¢ = kho·∫£ng c√°ch trung b√¨nh t·ª´ c√°c ƒëi·ªÉm trong c·ª•m i ƒë·∫øn t√¢m c·µ¢")
    print("   ‚Ä¢ DB c√†ng nh·ªè ‚Üí ph√¢n c·ª•m c√†ng t·ªët")
    
    if len(unique_clusters) >= 2:
        print("\n‚ñ∂ T√çNH CHI TI·∫æT:")
        
        # T√≠nh S cho m·ªói c·ª•m
        S = {}
        for j in range(k):
            members = data[labels == j]
            if len(members) > 0:
                dists = [euclid(p, centroids[j]) for p in members]
                S[j] = float(np.mean(dists))
            else:
                S[j] = 0
            print(f"\n   S[{j}] = kho·∫£ng c√°ch trung b√¨nh trong Cluster {j}")
            print(f"        = {S[j]:.6f}")
        
        # T√≠nh D cho m·ªói c·ª•m
        D_i = []
        for i in range(k):
            print(f"\n   T√≠nh D[{i}]:")
            max_r = 0
            for j in range(k):
                if j == i:
                    continue
                dist_c = euclid(centroids[i], centroids[j])
                if dist_c > 0:
                    r_ij = (S[i] + S[j]) / dist_c
                else:
                    r_ij = float("inf")
                print(f"      R[{i},{j}] = ({S[i]:.6f} + {S[j]:.6f}) / {dist_c:.6f} = {r_ij:.6f}")
                if r_ij > max_r:
                    max_r = r_ij
            D_i.append(max_r)
            print(f"      D[{i}] = max(R[{i},j]) = {max_r:.6f}")
        
        db_index = float(np.mean(D_i))
        print(f"\n‚ñ∂ DAVIES-BOULDIN INDEX:")
        print(f"   DB = (1/{k}) √ó ({' + '.join([f'{d:.6f}' for d in D_i])})")
        print(f"      = {db_index:.6f}")
    else:
        db_index = float("inf")
        print("\n   ‚ö† Ch·ªâ c√≥ 1 c·ª•m ‚Üí Davies-Bouldin = inf")
    
    # =============================================
    # B·∫¢NG T·ªîNG K·∫æT
    # =============================================
    print("\n" + "="*100)
    print("B·∫¢NG T·ªîNG K·∫æT K·∫æT QU·∫¢ PH√ÇN C·ª§M")
    print("="*100)
    
    print(f"\n{'Cluster':<10} {'S·ªë ƒëi·ªÉm':<10} {'Centroid':<30} {'Silhouette TB':<15}")
    print("-"*70)
    
    for j in range(k):
        cnt = int(np.sum(labels == j))
        c = centroids[j]
        sil_j = float(np.mean(s_values[labels == j])) if np.any(labels == j) else 0
        print(f"{j:<10} {cnt:<10} ({c[0]:.4f}, {c[1]:.4f}){'':<10} {sil_j:.6f}")
    
    print("-"*70)
    print(f"{'T·ªîNG':<10} {n_samples:<10} {'':<30} {sil_mean:.6f}")
    print(f"\nüìä Davies-Bouldin Index: {db_index:.6f}")
    print(f"   (DB c√†ng nh·ªè ‚Üí ph√¢n c·ª•m c√†ng t·ªët)")
    
    return labels, centroids, sil_mean, db_index

# ------------------------------------------------------
# T√≠nh MPNN distance v√† merge core-clusters (Step 5)
# ------------------------------------------------------
def mpnn_distance(clusterA, clusterB, dim):
    """MPNN distance gi·ªØa 2 core-cluster, ƒë√∫ng theo m√¥ t·∫£ paper.

    - M ƒë∆∞·ª£c t√≠nh theo k√≠ch th∆∞·ªõc hai cluster (Equation 1, x·∫•p x·ªâ):
        M = min( |A|^((T-1)/T), |B|^((T-1)/T) ) v·ªõi T = dim.
    - T√≠nh kho·∫£ng c√°ch gi·ªØa T·∫§T C·∫¢ c√°c c·∫∑p ƒëi·ªÉm c·ªßa hai cluster.
    - L·∫•y M kho·∫£ng c√°ch nh·ªè nh·∫•t r·ªìi tr·∫£ v·ªÅ trung b√¨nh ƒë∆°n gi·∫£n c·ªßa ch√∫ng.
    """

    # 1. T√≠nh k√≠ch th∆∞·ªõc hai cluster theo s·ªë ƒëi·ªÉm
    size_A = sum(len(g.get("points", [])) for g in clusterA)
    size_B = sum(len(g.get("points", [])) for g in clusterB)

    if size_A == 0 or size_B == 0:
        # N·∫øu m·ªôt trong hai cluster kh√¥ng c√≥ ƒëi·ªÉm, coi kho·∫£ng c√°ch l√† v√¥ c√πng
        print("[Step 5 - MPNN] M·ªôt trong hai cluster kh√¥ng c√≥ ƒëi·ªÉm, tr·∫£ v·ªÅ inf.")
        return float("inf")

    # 2. T√≠nh M theo c√¥ng th·ª©c x·∫•p x·ªâ trong paper
    T = dim
    exp_factor = (T - 1) / T if T > 0 else 0.0

    M_A = int(size_A ** exp_factor) if exp_factor > 0 else size_A
    M_B = int(size_B ** exp_factor) if exp_factor > 0 else size_B
    M = min(M_A, M_B)

    if M <= 0:
        M = 1

    # 3. T√≠nh kho·∫£ng c√°ch gi·ªØa T·∫§T C·∫¢ c√°c c·∫∑p ƒëi·ªÉm t·ª´ 2 cluster
    all_distances = []
    for g1 in clusterA:
        for p1 in g1.get("points", []):
            for g2 in clusterB:
                for p2 in g2.get("points", []):
                    d = euclid(p1, p2)
                    all_distances.append(d)

    if not all_distances:
        # Ph√≤ng tr∆∞·ªùng h·ª£p kh√¥ng sinh ƒë∆∞·ª£c kho·∫£ng c√°ch n√†o
        print("[Step 5 - MPNN] Kh√¥ng c√≥ c·∫∑p ƒëi·ªÉm n√†o gi·ªØa hai cluster, tr·∫£ v·ªÅ inf.")
        return float("inf")

    # 4. S·∫Øp x·∫øp v√† l·∫•y M kho·∫£ng c√°ch nh·ªè nh·∫•t
    all_distances.sort()
    top_M = all_distances[:M]

    # 5. T√≠nh trung b√¨nh (weighted average ƒë∆°n gi·∫£n)
    mpnn_value = sum(top_M) / len(top_M)
    print(f"[Step 5 - MPNN]   ==> MPNN distance (mean of top-{M}) gi·ªØa hai cluster = {mpnn_value:.2f}")
    return mpnn_value


def merge_core_clusters(core_clusters, dim, target_k=None):
    """Step 5: MPNN-based merging of core-clusters (paper-style).

    core_clusters: list c√°c cluster (m·ªói cluster l√† list c√°c core-grid)
    dim: s·ªë chi·ªÅu d·ªØ li·ªáu (T)
    target_k: n·∫øu ƒë·∫∑t, l·∫∑p merge t·ªõi khi len(clusters) <= target_k;
              n·∫øu None ‚Üí kh√¥ng merge, tr·∫£ v·ªÅ b·∫£n sao.
    """

    # Sao ch√©p shallow list ƒë·ªÉ kh√¥ng s·ª≠a tr·ª±c ti·∫øp ƒë·∫ßu v√†o
    clusters = [c[:] for c in core_clusters]

    # N·∫øu kh√¥ng y√™u c·∫ßu gi·∫£m s·ªë cluster th√¨ gi·ªØ nguy√™n
    if target_k is None:
        return clusters

    # L·∫∑p merge cho t·ªõi khi ƒë·∫°t target_k ho·∫∑c kh√¥ng merge ƒë∆∞·ª£c n·ªØa
    from itertools import combinations

    while len(clusters) > target_k:
        n = len(clusters)
        best_pair = None
        best_dist = float("inf")

        # Duy·ªát m·ªçi c·∫∑p cluster ƒë·ªÉ t√¨m c·∫∑p c√≥ MPNN nh·ªè nh·∫•t
        for i, j in combinations(range(n), 2):
            d = mpnn_distance(clusters[i], clusters[j], dim)
            if d < best_dist:
                best_dist = d
                best_pair = (i, j)

        if best_pair is None or not math.isfinite(best_dist):
            # Kh√¥ng t√¨m ƒë∆∞·ª£c c·∫∑p h·ª£p l·ªá ƒë·ªÉ merge (vd. m·ªçi kho·∫£ng c√°ch l√† inf)
            break

        i, j = best_pair
        merged = clusters[i] + clusters[j]

        # T·∫°o danh s√°ch cluster m·ªõi sau khi g·ªôp
        new_clusters = []
        for idx in range(n):
            if idx != i and idx != j:
                new_clusters.append(clusters[idx])
        new_clusters.append(merged)
        clusters = new_clusters

    return clusters


# ------------------------------------------------------
# V·∫Ω k·∫øt qu·∫£ Step 5: core-clusters sau MPNN (tr√™n l∆∞·ªõi ƒë·ªá quy)
# ------------------------------------------------------
def plot_step5_core_clusters(points, grid_list, merged_clusters,
                             title_prefix="B∆∞·ªõc 5: Core-Cluster sau gom MPNN"):
    """V·∫Ω c√°c core-grid tr√™n l∆∞·ªõi ƒë·ªá quy, t√¥ m√†u theo c·ª•m sau MPNN.

    - merged_clusters: list c√°c cluster, m·ªói cluster l√† list c√°c grid dict n·∫±m trong grid_list.
    - Non-core grid v·∫Ω x√°m nh·∫°t, core-grid ƒë∆∞·ª£c t√¥ m√†u theo id cluster.
    """
    # T√≠nh bounds tr·ª±c ti·∫øp t·ª´ d·ªØ li·ªáu ƒëi·ªÉm (gi·ªëng Step 1) ƒë·ªÉ tr·ª•c ƒë·ªìng nh·∫•t
    data = np.asarray(points)
    if len(data) > 0:
        xmin, xmax = float(data[:, 0].min()), float(data[:, 0].max())
        ymin, ymax = float(data[:, 1].min()), float(data[:, 1].max())
    else:
        # fallback t·ª´ grid_list n·∫øu kh√¥ng c√≥ ƒëi·ªÉm
        xs_min = [g["min_bin"][0] for g in grid_list]
        xs_max = [g["max_bin"][0] for g in grid_list]
        ys_min = [g["min_bin"][1] for g in grid_list]
        ys_max = [g["max_bin"][1] for g in grid_list]

        xmin, xmax = min(xs_min), max(xs_max)
        ymin, ymax = min(ys_min), max(ys_max)

    fig, ax = plt.subplots(figsize=(10, 8))

    # Map grid id -> m√†u c·ª•m (ch·ªâ v·ªõi core-grid)
    color_by_grid_id = {}
    for ci, cluster in enumerate(merged_clusters):
        color = CLUSTER_COLORS[ci % len(CLUSTER_COLORS)]
        for g in cluster:
            color_by_grid_id[id(g)] = color

    # V·∫Ω to√†n b·ªô grid
    for g in grid_list:
        x0, x1 = g["min_bin"][0], g["max_bin"][0]
        y0, y1 = g["min_bin"][1], g["max_bin"][1]

        is_core = g.get("is_core", False)
        if is_core and id(g) in color_by_grid_id:
            facecolor = color_by_grid_id[id(g)]
            alpha = 0.5
            edgecolor = "red"
        else:
            facecolor = "#f0f0f0"
            alpha = 0.15
            edgecolor = "red"

        rect = patches.Rectangle(
            (x0, y0),
            x1 - x0,
            y1 - y0,
            linewidth=1.0,
            edgecolor=edgecolor,
            facecolor=facecolor,
            alpha=alpha,
        )
        ax.add_patch(rect)

    # V·∫Ω to√†n b·ªô ƒëi·ªÉm d·ªØ li·ªáu (m√†u xanh d∆∞∆°ng)
    if len(data) > 0:
        ax.scatter(data[:, 0], data[:, 1], c="blue", s=15, zorder=10, label=f"Points ({len(data)})")

    # Tr·ª•c d√πng ƒë√∫ng bounds t·ª´ d·ªØ li·ªáu, kh√¥ng th√™m margin
    ax.set_xlim(xmin, xmax)
    ax.set_ylim(ymin, ymax)
    ax.set_aspect("equal")
    ax.set_xlabel("Tr·ª•c X", fontsize=11)
    ax.set_ylabel("Tr·ª•c Y", fontsize=11)
    ax.set_title(title_prefix, fontsize=13, fontweight="bold")

    # Legend: t·ª´ng core-cluster m·ªôt m√†u + non-core grid + ƒëi·ªÉm
    legend_elements = [
        patches.Patch(facecolor="#f0f0f0", edgecolor="red", alpha=0.15, label="Non-core grid"),
    ]
    for ci, cluster in enumerate(merged_clusters):
        color = CLUSTER_COLORS[ci % len(CLUSTER_COLORS)]
        legend_elements.append(
            patches.Patch(
                facecolor=color,
                edgecolor="red",
                alpha=0.5,
                label=f"Cluster {ci + 1} ({len(cluster)} core-grids)",
            )
        )
    legend_elements.append(
        patches.Patch(
            facecolor="blue",
            edgecolor="blue",
            alpha=0.8,
            label=f"Points ({len(points)})",
        )
    )
    ax.legend(
        handles=legend_elements,
        loc="upper left",
        bbox_to_anchor=(1.02, 1.0),
        borderaxespad=0.0,
        fontsize=9,
        framealpha=0.9,
    )

    ax.grid(True, alpha=0.2, linestyle="--")
    # Ch·ª´a th√™m l·ªÅ b√™n ph·∫£i cho legend ƒë·ªÉ tr√°nh c·∫£nh b√°o tight_layout
    plt.subplots_adjust(right=0.8)
    plt.show()


# ------------------------------------------------------
# V·∫Ω k·∫øt qu·∫£ Step 6: l∆∞·ªõi + core-grids + ƒëi·ªÉm t√¥ m√†u theo cluster
# ------------------------------------------------------
def plot_step5_clusters(points, grid_list, core_clusters, cluster_labels, final_centroids,
                        title_prefix="K·∫øt qu·∫£ cu·ªëi c√πng"):
    data = np.asarray(points)
    n_clusters = len(final_centroids)

    # T√≠nh bounds tr·ª±c ti·∫øp t·ª´ d·ªØ li·ªáu ƒëi·ªÉm (gi·ªëng Step 1)
    if len(data) > 0:
        xmin, xmax = float(data[:, 0].min()), float(data[:, 0].max())
        ymin, ymax = float(data[:, 1].min()), float(data[:, 1].max())
    else:
        xs_min = [g["min_bin"][0] for g in grid_list]
        xs_max = [g["max_bin"][0] for g in grid_list]
        ys_min = [g["min_bin"][1] for g in grid_list]
        ys_max = [g["max_bin"][1] for g in grid_list]

        xmin, xmax = min(xs_min), max(xs_max)
        ymin, ymax = min(ys_min), max(ys_max)

    # D√πng c√πng m·ªôt kho·∫£ng cho c·∫£ tr·ª•c X v√† Y
    global_min = min(xmin, ymin)
    global_max = max(xmax, ymax)

    fig, ax = plt.subplots(figsize=(10, 8))

    # M√†u cho cluster (√¥ l∆∞·ªõi + ƒëi·ªÉm) d√πng b·∫£ng m√†u chung
    cluster_colors = CLUSTER_COLORS

    # Map ƒëi·ªÉm -> nh√£n cluster
    label_by_point = {}
    for p, lab in zip(points, cluster_labels):
        label_by_point[tuple(p)] = int(lab)

    # V·∫Ω t·∫•t c·∫£ grid: g√°n cluster cho √¥ theo nh√£n chi·∫øm ƒëa s·ªë c·ªßa ƒëi·ªÉm trong √¥
    for g in grid_list:
        x0, x1 = g["min_bin"][0], g["max_bin"][0]
        y0, y1 = g["min_bin"][1], g["max_bin"][1]

        labels_in_cell = []
        for p in g.get("points", []):
            lab = label_by_point.get(tuple(p))
            if lab is not None:
                labels_in_cell.append(lab)

        if labels_in_cell:
            # Nh√£n chi·∫øm ƒëa s·ªë trong √¥
            counts = np.bincount(labels_in_cell)
            ci = int(np.argmax(counts))
            facecolor = cluster_colors[ci % len(cluster_colors)]
            alpha = 0.4
            edgecolor = "red"
        else:
            # √î kh√¥ng ch·ª©a ƒëi·ªÉm n√†o (ho·∫∑c ƒëi·ªÉm ch∆∞a g√°n cluster) -> x√°m nh·∫°t
            facecolor = "#f0f0f0"
            alpha = 0.15
            edgecolor = "red"

        rect = patches.Rectangle(
            (x0, y0),
            x1 - x0,
            y1 - y0,
            linewidth=0.5,
            edgecolor=edgecolor,
            facecolor=facecolor,
            alpha=alpha,
        )
        ax.add_patch(rect)

    # V·∫Ω ƒëi·ªÉm d·ªØ li·ªáu, t√¥ m√†u theo nh√£n cluster
    for ci in range(n_clusters):
        pts_ci = data[cluster_labels == ci]
        if len(pts_ci) == 0:
            continue
        ax.scatter(
            pts_ci[:, 0],
            pts_ci[:, 1],
            c=[cluster_colors[ci % len(cluster_colors)]],
            s=20,
            zorder=10,
            label=f"Cluster {ci + 1} ({len(pts_ci)})",
        )

    # C√†i ƒë·∫∑t tr·ª•c: d√πng ƒë√∫ng bounds t·ª´ d·ªØ li·ªáu, kh√¥ng th√™m margin
    ax.set_xlim(xmin, xmax)
    ax.set_ylim(ymin, ymax)
    ax.set_aspect("equal")
    ax.set_xlabel("Tr·ª•c X", fontsize=11)
    ax.set_ylabel("Tr·ª•c Y", fontsize=11)
    ax.set_title(title_prefix, fontsize=13, fontweight="bold")

    # Legend cho c√°c cluster v√† l∆∞·ªõi n·ªÅn (gi·ªëng v√≠ d·ª•)
    bg_patch = patches.Patch(
        facecolor="#f0f0f0",
        edgecolor="red",
        alpha=0.15,
        label="Grid background",
    )
    handles, labels = ax.get_legend_handles_labels()
    handles = [bg_patch] + handles
    ax.legend(
        handles=handles,
        loc="upper left",
        bbox_to_anchor=(1.02, 1.0),
        borderaxespad=0.0,
        fontsize=9,
        framealpha=0.9,
    )

    ax.grid(True, alpha=0.2, linestyle="--")
    # Ch·ª´a th√™m l·ªÅ b√™n ph·∫£i cho legend ƒë·ªÉ tr√°nh c·∫£nh b√°o tight_layout
    plt.subplots_adjust(right=0.8)
    plt.show()


# ------------------------------------------------------
# H√†m ch√≠nh STEP 5+6: MPNN merge r·ªìi K-means cho to√†n b·ªô ƒëi·ªÉm
# ------------------------------------------------------
def step5_cluster_full(points, grid_list, visualize=True, target_k=None):
    """Th·ª±c hi·ªán Step 5 v√† 6:

    - Step 5: Gom core-grids th√†nh c√°c core-cluster (theo adjacency Step 4)
      r·ªìi merge th√™m b·∫±ng MPNN distance v·ªõi ng∆∞·ª°ng theta.
    - Step 6: T√≠nh t√¢m ban ƒë·∫ßu t·ª´ c√°c core-cluster sau merge v√† ch·∫°y K-means
      c√†i tay cho to√†n b·ªô ƒëi·ªÉm (d√πng utils.euclid).
    """

    start_time = time.time()

    if not grid_list:
        raise ValueError("grid_list r·ªóng, kh√¥ng th·ªÉ ch·∫°y Step 5/6")

    dim = len(grid_list[0]["min_bin"])
    print(f"[Step 5] S·ªë √¥ l∆∞·ªõi ƒë·∫ßu v√†o Step 5: {len(grid_list)} (dim={dim})")

    # 1) Gom core-grids th√†nh core-clusters ban ƒë·∫ßu (Step 4 k·∫øt qu·∫£ ƒë∆∞a sang)
    core_clusters_raw = build_core_clusters(grid_list, dim)
    print(f"[Step 5] S·ªë core-grid: {sum(g.get('is_core', False) for g in grid_list)}")
    print(f"[Step 5] S·ªë core-cluster ban ƒë·∫ßu (t·ª´ adjacency): {len(core_clusters_raw)}")

    # 2) MPNN-based merging (Step 5)
    print(f"[Step 5 - MPNN] B·∫Øt ƒë·∫ßu merge theo paper-style, target_k={target_k}")
    merged_clusters = merge_core_clusters(core_clusters_raw, dim, target_k=target_k)
    if visualize:
        # V·∫Ω core-clusters sau MPNN tr∆∞·ªõc khi ch·∫°y K-means
        plot_step5_core_clusters(points, grid_list, merged_clusters,
                                 title_prefix="B∆∞·ªõc 5: Core-Cluster sau gom MPNN")

    # 3) T√≠nh t√¢m t·ª´ng cluster sau merge (l√†m t√¢m kh·ªüi t·∫°o cho K-means, Step 6)
    init_centroids = compute_initial_centroids(merged_clusters)
    print(f"[Step 6] S·ªë centroid kh·ªüi t·∫°o t·ª´ core-clusters sau merge: {0 if init_centroids is None else len(init_centroids)}")
    if init_centroids is None or len(init_centroids) == 0:
        print("\n[Step 6] Kh√¥ng c√≥ centroid core-cluster n√†o sau MPNN merge.")
        raise ValueError("Step 6: Kh√¥ng th·ªÉ ch·∫°y K-means v√¨ kh√¥ng c√≥ core-cluster centroid kh·ªüi t·∫°o.")

    # 4) Step 6: K-means cho to√†n b·ªô ƒëi·ªÉm (c√†i tay, d√πng euclid)
    data = np.asarray(points)
    print(f"[Step 6] B·∫Øt ƒë·∫ßu K-means cho to√†n b·ªô {len(data)} ƒëi·ªÉm.")
    cluster_labels, final_centroids = kmeans_assign_all_points_custom(data, init_centroids)
    print("[Step 6] Ho√†n th√†nh K-means.")
    print("[Step 6] K·∫æT QU·∫¢ THEO T·ª™NG CLUSTER:")
    for ci, c in enumerate(final_centroids):
        cnt = int(np.sum(cluster_labels == ci))
        print(f"  - Cluster {ci}: s·ªë ƒëi·ªÉm = {cnt}, centroid = {c}")

    # 4b) T√≠nh v√† in ch·ªâ s·ªë Silhouette
    sil_scores, sil_mean = compute_silhouette(points, cluster_labels)
    print("\n[Step 6] CH·ªà S·ªê SILHOUETTE:")
    print(f"  - Silhouette trung b√¨nh cho to√†n b·ªô ph√¢n c·ª•m = {sil_mean:.2f}")

    # Silhouette trung b√¨nh theo t·ª´ng cluster
    labels_arr = np.asarray(cluster_labels)
    n_clusters = len(final_centroids)
    cluster_silhouette_stats = []
    for ci in range(n_clusters):
        mask = labels_arr == ci
        if not np.any(mask):
            continue
        sil_ci = float(sil_scores[mask].mean())
        count_ci = int(np.sum(mask))
        cluster_silhouette_stats.append((ci, sil_ci, count_ci))
        print(f"  - Cluster {ci}: Silhouette trung b√¨nh = {sil_ci:.2f} (s·ªë ƒëi·ªÉm = {count_ci})")

    # M·ªôt v√†i ƒëi·ªÉm "kh√≥ ph√¢n c·ª•m" (Silhouette th·∫•p nh·∫•t)
    k = min(10, len(points))
    hard_points_info = []
    if k > 0:
        print("\n  - Top c√°c ƒëi·ªÉm c√≥ Silhouette th·∫•p nh·∫•t (kh√≥ ph√¢n c·ª•m):")
        idx_sorted = np.argsort(sil_scores)  # tƒÉng d·∫ßn
        for rank in range(k):
            i = int(idx_sorted[rank])
            p = points[i]
            ci = int(labels_arr[i])
            s_i = float(sil_scores[i])
            hard_points_info.append((i, p, ci, s_i))
            print(f"      + ƒêi·ªÉm {i}: p={p}, cluster={ci}, silhouette={s_i:.2f}")

    # Ghi k·∫øt qu·∫£ Silhouette ra file TXT
    # --- Compute Davies-Bouldin index for the clustering ---
    labels_arr = np.asarray(cluster_labels)
    db_index = compute_davies_bouldin(data, labels_arr, final_centroids)
    print(f"\n[Step 6] Davies-Bouldin index (to√†n h·ªá th·ªëng) = {db_index:.6f}")

    try:
        with open("silhouette_results_demo.txt", "w", encoding="utf-8") as f:
            f.write("[Step 6] CH·ªà S·ªê SILHOUETTE\n")
            f.write(f"Silhouette trung b√¨nh to√†n b·ªô: {sil_mean:.6f}\n\n")

            f.write("Silhouette trung b√¨nh theo t·ª´ng cluster:\n")
            for ci, sil_ci, count_ci in cluster_silhouette_stats:
                f.write(f"- Cluster {ci}: silhouette_mean={sil_ci:.6f}, so_diem={count_ci}\n")

            f.write("\nTop c√°c ƒëi·ªÉm c√≥ Silhouette th·∫•p nh·∫•t (kh√≥ ph√¢n c·ª•m):\n")
            for i, p, ci, s_i in hard_points_info:
                f.write(
                    f"+ Diem {i}: x={float(p[0]):.6f}, y={float(p[1]):.6f}, "
                    f"cluster={ci}, silhouette={s_i:.6f}\n"
                )
            # Write Davies-Bouldin index
            f.write(f"\nDavies-Bouldin = {db_index:.6f}\n")
        print("\n[Step 6] ƒê√£ ghi k·∫øt qu·∫£ Silhouette ra file silhouette_results_demo.txt")
    except Exception as e:
        print(f"\n[Step 6] L·ªói khi ghi file silhouette_results_caitien_demo.txt: {e}")

    # 5) V·∫Ω k·∫øt qu·∫£ cu·ªëi c√πng n·∫øu c·∫ßn (d·ª±a tr√™n format Step 2/3)
    if visualize:
        plot_step5_clusters(points, grid_list, merged_clusters, cluster_labels, final_centroids,
                            title_prefix="B∆∞·ªõc 6: K·∫øt qu·∫£ ph√¢n c·ª•m cu·ªëi c√πng")

    total_time = time.time() - start_time
    print(f"\n[Timing] Th·ªùi gian x·ª≠ l√Ω Step 5+6: {total_time:.4f} gi√¢y")

    return cluster_labels, merged_clusters, final_centroids, sil_mean, db_index, total_time


if __name__ == "__main__":
    """Demo ƒë·∫ßy ƒë·ªß pipeline: Step 1 -> 2 -> 3 -> 4 -> 5.

    Ch·ªâ d√πng khi ch·∫°y tr·ª±c ti·∫øp file step5_core_clustering.py.
    """
    
    # ========= C·∫§U H√åNH =========
    # ƒê·∫∑t True ƒë·ªÉ hi·ªÉn th·ªã ƒë·ªì th·ªã, False ƒë·ªÉ t·∫Øt (tr√°nh ch·ªù ƒë√≥ng c·ª≠a s·ªï)
    SHOW_PLOTS = True
    # ============================

    from step1_compute_M_R import step1_compute_original
    from utils import load_data_txt
    from step3_recursive_partitioning import step3_handle_dense_grids
    from step2_grid_classification import build_grid, classify_grids, plot_classification

    data_path = "data.txt"

    # ƒêo th·ªùi gian cho to√†n b·ªô h·ªá th·ªëng (pipeline Step 1 -> 6)
    total_start = time.time()

    # 1) Step 1: t√¨m M, R
    print("===== STEP 1: T√≠nh M, R =====")
    step1_result = step1_compute_original(data_path, K=10, max_M=200)
    M = step1_result["M"]
    R = step1_result["R"]
    print(f"[Step 1] M = {M}, R = {R}")

    # 2) ƒê·ªçc d·ªØ li·ªáu
    print("\n===== ƒê·ªåC D·ªÆ LI·ªÜU =====")
    points = load_data_txt(data_path)
    print(f"[Data] S·ªë ƒëi·ªÉm ƒë·ªçc ƒë∆∞·ª£c: {len(points)}")

    # 2) Step 2: l∆∞·ªõi tƒ©nh + ph√¢n lo·∫°i
    print("\n===== STEP 2: X√¢y l∆∞·ªõi tƒ©nh v√† ph√¢n lo·∫°i =====")
    grid_step2, bounds = build_grid(points, M)
    print(f"[Step 2] S·ªë √¥ l∆∞·ªõi tƒ©nh: {len(grid_step2)}")
    classified_step2 = classify_grids(grid_step2, R)
    for gtype in ["core", "dense", "sparse", "empty"]:
        cells = classified_step2.get(gtype, [])
        print(f"[Step 2] S·ªë √¥ lo·∫°i {gtype}: {len(cells)}")
    if SHOW_PLOTS:
        plot_classification(points, grid_step2, classified_step2, bounds, M, R)

    # 3) Step 3: x√¢y l∆∞·ªõi ƒë·ªá quy v√† ph√¢n lo·∫°i core/dense/sparse/empty
    print("\n===== STEP 3: L∆∞·ªõi ƒë·ªá quy v√† ph√¢n lo·∫°i =====")
    step3_result = step3_handle_dense_grids(points, M, R, bounds, visualize=SHOW_PLOTS)
    final_cells = step3_result["final_cells"]
    print(f"[Step 3] S·ªë √¥ cu·ªëi c√πng (final_cells): {len(final_cells)}")

    # 4) Step 4: Gom core-grids tr√™n l∆∞·ªõi tƒ©nh Step 2 v√† v·∫Ω
    print("\n===== STEP 4: Gom core-grids tr√™n l∆∞·ªõi tƒ©nh Step 2 =====")
    grid_list_step4 = []
    for (ix, iy), cell in grid_step2.items():
        # L∆∞u √Ω: sau refactor, √¥ l∆∞·ªõi Step 2 l√† GridCell, kh√¥ng c√≤n l√† dict.
        # L·∫•y lo·∫°i √¥ t·ª´ thu·ªôc t√≠nh grid_type, n·∫øu ch∆∞a c√≥ th√¨ coi l√† 'unclassified'.
        gtype = getattr(cell, "grid_type", "unclassified")
        is_core = gtype == "core" or cell in classified_step2.get("core", [])

        # Chuy·ªÉn v·ªÅ dict v·ªõi min_bin/max_bin/points/is_core gi·ªëng format Step 4/5 d√πng chung.
        grid_list_step4.append(
            {
                "ix": ix,
                "iy": iy,
                "min_bin": (cell.xmin, cell.ymin),
                "max_bin": (cell.xmax, cell.ymax),
                "points": list(getattr(cell, "points", [])),
                "is_core": is_core,
            }
        )

    core_clusters_step4 = build_core_clusters(grid_list_step4, dim=2)
    print(f"[Step 4] S·ªë core-grids tr√™n l∆∞·ªõi tƒ©nh: {sum(g['is_core'] for g in grid_list_step4)}")
    print(f"[Step 4] S·ªë core-cluster (Step 4): {len(core_clusters_step4)}")
    if SHOW_PLOTS:
        plot_core_groups(points, grid_list_step4, core_clusters_step4,
                         title_prefix="B∆∞·ªõc 4: Gom Core Grid th√†nh Cluster")

    # 5) Step 5+6: Gom core-grids t·ª´ l∆∞·ªõi ƒë·ªá quy Step 3 + MPNN + K-means to√†n b·ªô ƒëi·ªÉm
    print("\n===== STEP 5+6: MPNN merge + K-means to√†n b·ªô ƒëi·ªÉm =====")
    grid_list_step5 = []
    for cell in final_cells:
        is_core = getattr(cell, "grid_type", None) == "core"
        grid_list_step5.append(
            {
                "min_bin": (cell.xmin, cell.ymin),
                "max_bin": (cell.xmax, cell.ymax),
                "points": list(getattr(cell, "points", [])),
                "is_core": is_core,
            }
        )

    cluster_labels, core_clusters, final_centroids, sil_mean, db_index, step56_time = step5_cluster_full(
        points, grid_list_step5, visualize=SHOW_PLOTS
    )

    print("\n===== K·∫æT QU·∫¢ STEP 6 (sau MPNN + K-means) =====")
    print(f"S·ªë cluster (t·ª´ core-clusters sau merge): {len(core_clusters)}")
    print(f"S·ªë ƒëi·ªÉm: {len(points)}")
    print(f"Silhouette trung b√¨nh to√†n b·ªô: {sil_mean:.4f}")
    print(f"Davies-Bouldin index to√†n h·ªá th·ªëng: {db_index:.6f}")
    print(f"Th·ªùi gian Step 5+6 (b√™n trong h√†m): {step56_time:.4f} gi√¢y")
    print("Centroids cu·ªëi c√πng:")
    for idx, c in enumerate(final_centroids):
        print(f"  Cluster {idx}: centroid = {c}")

    total_end = time.time()
    total_runtime = total_end - total_start
    print(f"\n[Timing] Th·ªùi gian ch ·∫°y TO√ÄN B·ªò H·ªÜ TH·ªêNG (Step 1 -> 6): {total_runtime:.4f} gi√¢y")

    # Ghi ra file TXT ch·ªâ ch·ª©a 2 ch·ªâ s·ªë: Time(s) to√†n b·ªô h·ªá th·ªëng v√† Silhouette trung b√¨nh
    try:
        with open("time_silhouette_results_demo.txt", "w", encoding="utf-8") as f:
            f.write("Time(s)_full_system = {:.6f}\n".format(total_runtime))
            f.write("Silhouette_mean = {:.6f}\n".format(sil_mean))
            f.write("Davies_Bouldin = {:.6f}\n".format(db_index))
        print("[Output] ƒê√£ ghi 2 ch·ªâ s·ªë Time(s) v√† Silhouette v√†o file time_silhouette_result_demo.txt")
    except Exception as e:
        print(f"[Output] L·ªói khi ghi file time_silhouette_result_demo.txt: {e}")
